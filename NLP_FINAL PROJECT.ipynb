{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8bd7f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\harip\\anaconda3\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\harip\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c177bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfcf469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 164456396\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'od. I see the success you got poppin in yo area.\\nRT : Consumers are visual. They want data at their finger tips. Mobile is the only way to deliver this, 24/7.\\nu welcome\\nIt is #RHONJ time!!\\nThe key to keeping your woman happy= attention, affection, treat her like a queen and sex her like a pornstar!\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"en_US.twitter.txt\", \"r\",encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:100])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b12301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    Args:\n",
    "    data: str\n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split(\"\\n\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0] \n",
    "    return sentences   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d03ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thunder and lightning.', 'Enter three Witches.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "x = \"Thunder and lightning.\\nEnter three Witches.\"\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c80480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        # append the list of words to the list \n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29f8b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green', '.'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "149495c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1867c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2360148/2360148 [04:12<00:00, 9364.66it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = get_tokenized_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fc950a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)# changes the original list/tuple/string\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c121e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360148 data are split into 1888118 train and 472030 test set\n",
      "First training sample:\n",
      "['yep', '!', 'about', 'a', 'year', 'ago', 'now', 'i', 'moved', 'down', 'to', 'ca', '.', 'i', 'like', 'it', '!']\n",
      "First test sample\n",
      "['q.', 'what', \"'s\", 'the', 'definition', 'of', 'a', 'quarter', 'tone', '?', 'a.', 'a', 'bagpiper', 'tuning', 'his', 'drones', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data are split into {} train and {} test set\".format(len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87c3966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):   \n",
    "    word_counts = {}\n",
    "    for sentence in range(len(tokenized_sentences)): \n",
    "        # go through tokens in the sentence\n",
    "        for token in (tokenized_sentences[sentence]): \n",
    "            if token not in word_counts.keys(): \n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa049661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    # appear at least 'minimum_freq' times.\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    for word, cnt in word_counts.items(): \n",
    "\n",
    "        if cnt>=count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0332051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    vocabulary = set(vocabulary)\n",
    "\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    # go through sentences\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        # go through tokens\n",
    "        for token in sentence: \n",
    "            # Is the token in the closed vocabulary?\n",
    "            if token in vocabulary:\n",
    "                # If so, append the word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                # otherwise, append the unknown token \n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)    \n",
    "    return replaced_tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11ebf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
    "    train_data_replaced=[]\n",
    "    test_data_replaced=[]\n",
    "    # replace less common words with \"<unk>\" training set\n",
    "    for sentence in tqdm(range(len(train_data))):\n",
    "        parole=[]\n",
    "        for word in range(len(train_data[sentence])):\n",
    "            if train_data[sentence][word] in vocabulary:\n",
    "                parole.append(train_data[sentence][word])\n",
    "            else:\n",
    "                parole.append(\"<unk>\")\n",
    "                \n",
    "        train_data_replaced.append(parole)\n",
    "    # replace less common words with \"<unk>\" test set\n",
    "    for sentence in tqdm(range(len(test_data))):\n",
    "        parole_test=[]\n",
    "        for word in range(len(test_data[sentence])):\n",
    "            if test_data[sentence][word] in vocabulary:\n",
    "                parole_test.append(test_data[sentence][word])\n",
    "            else:\n",
    "                parole_test.append(\"<unk>\")\n",
    "        test_data_replaced.append(parole_test)\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17b711f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1888118/1888118 [1:31:45<00:00, 342.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 472030/472030 [24:17<00:00, 323.95it/s]\n"
     ]
    }
   ],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data,minimum_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e28eb6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['yep', '!', 'about', 'a', 'year', 'ago', 'now', 'i', 'moved', 'down', 'to', 'ca', '.', 'i', 'like', 'it', '!']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['q.', 'what', \"'s\", 'the', 'definition', 'of', 'a', 'quarter', 'tone', '?', 'a.', 'a', 'bagpiper', 'tuning', 'his', 'drones', '.']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['yep', '!', 'about', 'a', 'year', 'ago', 'now', 'i', 'moved', 'down']\n",
      "\n",
      "Size of vocabulary: 146227\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f27306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = {}\n",
    "    for sentence in range(len(data)):\n",
    "        sentences = [start_token]*n+list(data[sentence])+[end_token]\n",
    "        sentences = tuple(sentences)\n",
    "        for i in range(0,len(sentences)-n+1):\n",
    "            n_gram = sentences[i:i+n]\n",
    "           # Is the n-gram in the dictionary?\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "               # otherwise is 1\n",
    "                n_grams[n_gram] = 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7b4015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39da7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    if type(previous_n_gram)==list:\n",
    "        previous_n_grams = tuple(previous_n_gram)\n",
    "    else:\n",
    "        previous_n_grams = tuple([previous_n_gram])\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_grams,0)\n",
    "    # and apply k-smoothing\n",
    "    denominator = previous_n_gram_count+vocabulary_size*1\n",
    "    n_plus1_gram =previous_n_grams+tuple([word])\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram,0)\n",
    "        \n",
    "    # and apply smoothing\n",
    "    numerator = n_plus1_gram_count+k\n",
    "    probability = numerator/denominator\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e38f8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    " \n",
    "    previous_n_gram = previous_n_gram\n",
    "    # add <e> <unk> to the vocabulary <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e10934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eba0a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "   count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "   count_matrix += k\n",
    "   prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "   return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aace8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentences, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "   n = len(list(n_gram_counts.keys())[0]) \n",
    "   sentence = [\"<s>\"] * n + sentences + [\"<e>\"]\n",
    "\n",
    "   sentence = tuple(sentence)\n",
    "   N = len(sentence)\n",
    "   product_pi = 1.0\n",
    "   for t in range(n, N): # complete this line\n",
    "\n",
    "       n_gram = sentence[t-n]\n",
    "       word = sentence[t]\n",
    "       probability = estimate_probability(word, n_gram, \n",
    "                        n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "       \n",
    "    \n",
    "       product_pi *= 1/probability\n",
    "\n",
    "   # Nth root of the product\n",
    "   perplexity = product_pi**(1/float(N))\n",
    "   perplexity = float(perplexity)\n",
    "\n",
    "   return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38785d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "   n = len(list(n_gram_counts.keys())[0]) \n",
    "   probabilities = estimate_probabilities(previous_tokens,\n",
    "                                          n_gram_counts, n_plus1_gram_counts,\n",
    "                                          vocabulary, k=k)\n",
    "   suggestion = None    \n",
    "   max_prob = 0\n",
    "   for word, prob in probabilities.items(): \n",
    "       if start_with != None : # complete this line\n",
    "           if word.startswith(start_with)==False: \n",
    "               #If so, move onto the next word\n",
    "               continue \n",
    "       if prob>max_prob: \n",
    "           suggestion = word\n",
    "           max_prob = prob\n",
    "\n",
    "   return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37ba7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `like` with a probability of 0.1111\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.1111\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "#Example starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3198071",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e20d0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5eef9cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('yep', 6.83858878881754e-06),\n",
       " ('yep', 6.83858878881754e-06),\n",
       " ('have', 9.567089213106912e-05),\n",
       " ('yep', 6.83858878881754e-06)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest1 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec6f4521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('yep', 6.83858878881754e-06),\n",
       " ('yep', 6.83858878881754e-06),\n",
       " ('yep', 6.83858878881754e-06),\n",
       " ('?', 0.00017088992638061973)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest2 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f688c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('down', 6.83858878881754e-06),\n",
       " ('down', 6.83858878881754e-06),\n",
       " ('down', 6.83858878881754e-06),\n",
       " ('doing', 6.15203734970231e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8dc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
